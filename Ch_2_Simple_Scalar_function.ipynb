{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa366d9-dfaf-4a22-8414-4a89fe9de39a",
   "metadata": {},
   "source": [
    "Manual vs. autograd\n",
    "\n",
    "Define a simple scalar function \n",
    "ùëì\n",
    "(\n",
    "ùë•\n",
    ",\n",
    "ùë¶\n",
    ")\n",
    "=\n",
    "(\n",
    "ùë•\n",
    "2\n",
    "ùë¶\n",
    "+\n",
    "sin\n",
    "‚Å°\n",
    "ùë•\n",
    ")\n",
    "f(x,y)=(x\n",
    "2\n",
    "y+sinx).\n",
    "\n",
    "Compute gradients two ways: (a) by hand with calculus; (b) with requires_grad=True and .backward().\n",
    "\n",
    "Verify numerically with finite differences.\n",
    "\n",
    "Deliverable: print grads, assert closeness, brief note on computational graph & .grad lifetimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5247d97-9088-4707-b34a-9794fa9e1bbc",
   "metadata": {},
   "source": [
    "Scalar fn:\n",
    "f(x,y)=(x 2 y+sinx)\n",
    "\n",
    "By hand:\n",
    "dy/dx = 2xy + cos(x)\n",
    "dx/dy = x2\n",
    "\n",
    "if x=1, y=2\n",
    "dy/dx = 4 + cos(1) = 4\n",
    "dx/dy = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09e578a2-42aa-4bf0-b89e-5146370bd1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.5403)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# requires_grad=True\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "fn = x**2 * y + torch.sin(x)\n",
    "fn.backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6d46a7-2bf7-4b05-af13-5961cdca6de0",
   "metadata": {},
   "source": [
    "Notes on autograd & .grad\n",
    "\n",
    "PyTorch builds a computational graph dynamically as you compute f(x,y).\n",
    "\n",
    "Calling .backward() computes gradients along this graph.\n",
    "\n",
    ".grad is populated only for leaf tensors with requires_grad=True.\n",
    "\n",
    "By default, .grad accumulates; call .zero_() to reset if reusing tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d12c1d47-0a34-479f-a40d-8e7669710b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grads = torch.autograd.grad(outputs=fn, inputs=(x, y))\n",
    "\n",
    "# dx, dy = grads\n",
    "# print(\"df/dx =\", dx)\n",
    "# print(\"df/dy =\", dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f6b02db-c6e2-4105-91c9-796bde2c9070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "x_val = x.item()\n",
    "y_val = y.item()\n",
    "eps = 1e-6\n",
    "\n",
    "df_dx_fd = ((x_val + eps)**2 * y_val + math.sin(x_val + eps) -\n",
    "            ((x_val - eps)**2 * y_val + math.sin(x_val - eps))) / (2*eps)\n",
    "\n",
    "df_dy_fd = ((x_val**2 * (y_val + eps) + math.sin(x_val)) -\n",
    "            (x_val**2 * (y_val - eps) + math.sin(x_val - eps))) / (2*eps)\n",
    "\n",
    "\n",
    "print(torch.isclose(x.grad, torch.tensor(df_dx_fd), atol=1e-6))  # True\n",
    "print(torch.isclose(y.grad, torch.tensor(df_dy_fd), atol=1e-6))  # True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
